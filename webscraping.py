# -*- coding: utf-8 -*-
"""webScraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zmgyi7z5PyBJ0FSv7TPoxxutL1x97pYP

# Você não precisa executar esse código se fizer isso no Jupyter Notebook ou em outras configurações locais do Python.
"""

# Commented out IPython magic to ensure Python compatibility.
# # executar esse script somente no google colab
# %%shell
# sudo apt -y update
# sudo apt install -y wget curl unzip
# wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb
# dpkg -i libu2f-udev_1.1.4-1_all.deb
# wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
# dpkg -i google-chrome-stable_current_amd64.deb
# CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`
# wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/
# unzip -o /tmp/chromedriver_linux64.zip -d /tmp/
# chmod +x /tmp/chromedriver
# mv /tmp/chromedriver /usr/local/bin/chromedriver
# pip install selenium
# pip install chromedriver-autoinstaller # caso o script mosk executar essa linha separadamente usando !pip

"""# Explicando o script

## sudo apt -y update

* Esta linha usa o comando sudo para executar uma tarefa com privilégios de superusuário (acesso de administrador).
apt é o gerenciador de pacotes para sistemas baseados em Debian, como o Ubuntu. A opção -y informa ao apt para assumir "sim" para qualquer prompt durante o processo de atualização.
Essa linha atualiza a lista de pacotes disponíveis em seu sistema.

## sudo apt install -y wget curl unzip


### Esta linha instala três pacotes específicos:

- wget: Um utilitário de linha de comando para baixar arquivos da internet.

- curl: Outra ferramenta de linha de comando para transferência de dados, frequentemente usada para downloads.

- unzip: Uma ferramenta para descompactar arquivos compactados no formato ZIP.

## wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb

* Esta linha usa o wget (instalado anteriormente) para baixar um arquivo específico. A URL aponta para um arquivo de pacote Debian (libu2f-udev_1.1.4-1_all.deb).

* Este pacote pode ser uma dependência necessária para algumas funcionalidades do Selenium no colab.

## dpkg -i libu2f-udev_1.1.4-1_all.deb

* Esta linha usa o dpkg (ferramenta do gerenciador de pacotes) para instalar o arquivo de pacote baixado (libu2f-udev_1.1.4-1_all.deb). A opção -i especifica que queremos instalar um pacote.

## wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb

* Esta linha baixa a versão estável mais recente do pacote deb do Google Chrome usando wget. A URL aponta para o repositório oficial do Google Chrome. Este script assume um sistema de 64 bits (amd64).

## dpkg -i google-chrome-stable_current_amd64.deb

* Esta linha instala o pacote baixado do Google Chrome (google-chrome-stable_current_amd64.deb) usando dpkg.

## CHROME_DRIVER_VERSION=$(curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE)

* Esta linha recupera a versão mais recente do driver Chrome usando curl (outra ferramenta de download com modo silencioso -sS). O número da versão é armazenado em uma variável de ambiente chamada CHROME_DRIVER_VERSION.

## wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/

* Esta linha baixa o driver Chrome apropriado para a versão recuperada ($CHROME_DRIVER_VERSION) usando wget. A opção -N garante que o download só ocorra se o arquivo ainda não existir (-N significa "sem sobrescrever"). O arquivo baixado é colocado no diretório /tmp (-P /tmp/).

## unzip -o /tmp/chromedriver_linux64.zip -d /tmp/

* Esta linha descompacta o arquivo compactado do driver Chrome (chromedriver_linux64.zip) usando unzip. A opção -o permite sobrescrever arquivos existentes com o mesmo nome.
Os arquivos extraídos são colocados no diretório /tmp (-d /tmp/).

## chmod +x /tmp/chromedriver

* Esta linha muda as permissões do arquivo chromedriver extraído usando chmod. A opção +x torna o arquivo executável, permitindo que seja executado na linha de comando.

## mv /tmp/chromedriver /usr/local/bin/chromedriver

* Esta linha move o arquivo chromedriver do diretório /tmp

# Usado para estudo
"""

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from time import sleep
from bs4 import BeautifulSoup
import pandas as pd
import openpyxl

import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

import chromedriver_autoinstaller

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

# set path to chromedriver as per your configuration
chromedriver_autoinstaller.install()

lista_links = [10323, 10325, 10327, 10343, 10345, 10347]


# set up the webdriver
navegador = webdriver.Chrome(options=chrome_options)

listaDfPrincipal = []

posDfPrincipal = 0
for i in lista_links:
    navegador.get('http://qselecao.ifce.edu.br/resultado_nomes.aspx?COD_REGRA_CLASSIFICACAO='+str(i))
    sleep(1)
    botaoClassificacao = navegador.find_element(By.ID, 'ctl00_ContentPlaceHolderPrincipal_rblOrdenacao_1')
    botaoClassificacao.click()
    sleep(1)
    soup = BeautifulSoup(navegador.page_source, 'html.parser')
    tabela = soup.find(id='ctl00_ContentPlaceHolderPrincipal_wucResultados1_grvConsulta')
    listaDataframes = pd.read_html(str(tabela))
    listaDfPrincipal.append(listaDataframes[0])
    sleep(1)
    for a in range(8):
        botaoCotas = navegador.find_element(By.ID, 'ctl00_ContentPlaceHolderPrincipal_wucSelecionarCota1_rblCotas_'+str(a+1))
        botaoCotas.click()
        page = navegador.page_source
        soup = BeautifulSoup(page, 'html.parser')
        tabelaCotas = soup.find(id='ctl00_ContentPlaceHolderPrincipal_wucResultados1_grvConsulta')
        if tabelaCotas != None:
            dfTabelaCotas = pd.read_html(str(tabelaCotas))
            listaDfPrincipal[posDfPrincipal] = pd.concat([listaDfPrincipal[posDfPrincipal], dfTabelaCotas[0]])
    sleep(1)
    posDfPrincipal += 1

navegador.close()

nomesPlanilhas = ['Edificações',  'Eletrotécnica', 'Informática', 'Mecânica Industrial', 'Química', 'Telecomunicações']

posNomes = 0
with pd.ExcelWriter("resultado.xlsx", mode="w", engine="openpyxl", ) as writer:
    for df in listaDfPrincipal:
        df.to_excel(writer, sheet_name=nomesPlanilhas[posNomes], index=False)
        posNomes += 1
# Com isso não será mais necessário criar o arquivo manualmente

"""# Insere o caminho especificado à busca de módulos: Esta linha garante que o script encontre o arquivo chromedriver na pasta indicada, onde está o driver necessário para controlar o navegador Chrome pelo Selenium.

"""

import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

"""# Imports

## Importa o módulo time, que oferece funções para tratar tempo e pausas.


## Importa o módulo Pandas como pd, uma biblioteca muito popular para manipulação de dados em forma de tabelas (DataFrames).

## Importa a classe BeautifulSoup do módulo bs4, uma biblioteca amplamente utilizada para analisar e extrair dados de HTML e XML.


## Importa a classe webdriver do módulo selenium, que é a principal ferramenta para controlar navegadores de forma automatizada.

## Importa o módulo chromedriver_autoinstaller, usado para instalar e gerenciar automaticamente o chromedriver se necessário.
"""

import time
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
import chromedriver_autoinstaller

"""# Configurações do webdriver

## chrome_options = webdriver.ChromeOptions()

* Cria um objeto ChromeOptions para configurar opções personalizadas do navegador Chrome.

##chrome_options.add_argument('--headless')

* Adiciona o argumento --headless às opções do Chrome, o que significa que o navegador funcionará em modo headless, ou seja, sem interface gráfica visível.

## chrome_options.add_argument('--no-sandbox')

* Adiciona o argumento --no-sandbox, utilizado para executar o Chrome em alguns ambientes específicos, como containers Docker.

## chrome_options.add_argument('--disable-dev-shm-usage')

* Adiciona o argumento --disable-dev-shm-usage para gerenciar a memória compartilhada, podendo prevenir erros em alguns sistemas.

## chromedriver_autoinstaller.install()

* Executa a instalação automática do chromedriver, garantindo que o driver esteja disponível na versão adequada.

## url = "https://onde_farei_scraping.com"

* Define a variável url com o endereço da página web a ser raspada (scrapeada).

## driver = webdriver.Chrome(options=chrome_options)

* Cria uma instância da classe webdriver.Chrome, iniciando o navegador Chrome com as opções especificadas anteriormente, pronto para ser controlado pelo script.


"""

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

chromedriver_autoinstaller.install()

url = "https://onde_farei_scraping.com"

driver = webdriver.Chrome(options=chrome_options)

"""# WebScraping Estático"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from time import sleep
from bs4 import BeautifulSoup
import chromedriver_autoinstaller

import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

# ------------------ Dependencias --------------------------

# Configurando o WebDriver

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless') # ensure GUI is off
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

# definindo o caminho para chromedriver de acordo com sua configuração
chromedriver_autoinstaller.install()

# configura o webdriver
navegador = webdriver.Chrome(options=chrome_options)


# URL da página alvo
url = "https://www.sspds.ce.gov.br/estatisticas-2-3/"

# Lista para armazenar os links dos PDFs
pdf_links = []
excel_links = []

# Obtendo o conteúdo da página
navegador.get(url)
soup = BeautifulSoup(navegador.page_source, 'html.parser')

# Encontrando todos os links na página
links = soup.find_all('a', class_='box')

# Filtrando apenas os links que contêm meses e anos
for link in links:
    if any(mes_ano in link['href'] for mes_ano in ["Janeiro", "Fevereiro", "Março", "Abril", "Maio", "Junho", "Julho", "Agosto", "Setembro", "Outubro", "Novembro", "Dezembro"]):
        pdf_links.append(link['href'])
    elif "CRIMES VIOLENTOS LETAIS E INTENCIONAIS – CVLI" in link.text:
        if link['href'].endswith('.xlsx'):
            excel_links.append(link['href'])

# Iterando sobre os links dos PDFs
for pdf_link in pdf_links:
    navegador.get(pdf_link)
    sleep(1)
    download_link = navegador.find_element(By.CSS_SELECTOR, 'a[href$=".pdf"]')
    sleep(5)

# Iterar sobre os links das planilhas Excel
for excel_link in excel_links:
    navegador.get(excel_link)
    sleep(1)
    download_link = navegador.find_element(By.CSS_SELECTOR, 'a[href$=".xlsx"]')
    sleep(5)

navegador.quit()

"""# WebScraping Dinâmico"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from time import sleep
from bs4 import BeautifulSoup
import chromedriver_autoinstaller

import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

# Configurando o WebDriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless') # garantir que a GUI está desativada
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

# Definindo o caminho para o chromedriver de acordo com sua configuração
chromedriver_autoinstaller.install()

# Configurando o webdriver
navegador = webdriver.Chrome(options=chrome_options)

# URL da página alvo
url = "https://www.sspds.ce.gov.br/estatisticas-2-3/"

# Lista para armazenar os links dos PDFs e Planilhas
pdf_links = []
excel_links = []

# Obtendo o conteúdo da página
navegador.get(url)
soup = BeautifulSoup(navegador.page_source, 'html.parser')

# Encontrando todos os links na página
links = soup.find_all('a', class_='box')

# Iterar sobre os links e coletar os URLs dos arquivos PDF e planilhas Excel relacionadas a CVLI
for link in links:
    if any(mes_ano in link['href'] for mes_ano in ["Janeiro", "Fevereiro", "Março", "Abril", "Maio", "Junho", "Julho", "Agosto", "Setembro", "Outubro", "Novembro", "Dezembro"]):
        pdf_links.append(link['href'])
    elif "CRIMES VIOLENTOS LETAIS E INTENCIONAIS – CVLI" in link.text:
        if link['href'].endswith('.xlsx'):
            excel_links.append(link['href'])

# Espera explícita para o elemento "INDICADORES - 2009 A 2023"
try:
    indicadores_link = WebDriverWait(navegador, 10).until(
        EC.presence_of_element_located((By.LINK_TEXT, "INDICADORES - 2009 A 2023"))
    )
    navegador.execute_script("arguments[0].click();", indicadores_link)

    # Extrair os links dos arquivos .xlsx da página de indicadores
    soup = BeautifulSoup(navegador.page_source, 'html.parser')
    indicadores_links = soup.find_all('a', class_='box')
    for link in indicadores_links:
        if "CRIMES VIOLENTOS LETAIS E INTENCIONAIS – CVLI" in link.text:
            if link['href'].endswith('.xlsx'):
                excel_links.append(link['href'])

    # Iterar sobre os links dos PDFs
    for pdf_link in pdf_links:
        navegador.get(pdf_link)
        sleep(1)
        download_link = navegador.find_element(By.CSS_SELECTOR, 'a[href$=".pdf"]')
        sleep(5)

    # Iterar sobre os links das planilhas Excel
    for excel_link in excel_links:
        navegador.get(excel_link)
        sleep(1)
        download_link = navegador.find_element(By.CSS_SELECTOR, 'a[href$=".xlsx"]')
        sleep(5)

finally:
    navegador.quit()
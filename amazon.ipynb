{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Va8JykIQblJ",
        "outputId": "959053d9-d100-4351-fde6-d590c5efe9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 5,482 B/110\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,920 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n",
            "Fetched 3,510 kB in 2s (1,456 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.21.2-2ubuntu1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "--2024-04-01 10:52:56--  http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 185.125.190.39, 185.125.190.36, 91.189.91.81, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|185.125.190.39|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3708 (3.6K) [application/x-debian-package]\n",
            "Saving to: ‘libu2f-udev_1.1.4-1_all.deb’\n",
            "\n",
            "libu2f-udev_1.1.4-1 100%[===================>]   3.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-01 10:52:56 (345 MB/s) - ‘libu2f-udev_1.1.4-1_all.deb’ saved [3708/3708]\n",
            "\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack libu2f-udev_1.1.4-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.4-1) ...\n",
            "Setting up libu2f-udev (1.1.4-1) ...\n",
            "--2024-04-01 10:52:57--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 173.194.218.91, 173.194.218.190, 173.194.218.93, ...\n",
            "Connecting to dl.google.com (dl.google.com)|173.194.218.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107252688 (102M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 102.28M   196MB/s    in 0.5s    \n",
            "\n",
            "2024-04-01 10:52:58 (196 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [107252688/107252688]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 121757 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (123.0.6312.86-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "--2024-04-01 10:53:13--  https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 173.194.213.207, 173.194.215.207, 173.194.216.207, ...\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|173.194.213.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7407250 (7.1M) [application/zip]\n",
            "Saving to: ‘/tmp/chromedriver_linux64.zip’\n",
            "\n",
            "chromedriver_linux6 100%[===================>]   7.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-04-01 10:53:13 (236 MB/s) - ‘/tmp/chromedriver_linux64.zip’ saved [7407250/7407250]\n",
            "\n",
            "Archive:  /tmp/chromedriver_linux64.zip\n",
            "  inflating: /tmp/chromedriver       \n",
            "  inflating: /tmp/LICENSE.chromedriver  \n",
            "Collecting selenium\n",
            "  Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.19.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# EXECUTAR ESSE SCRIPT SOMENTE NO GOOGLE COLAB\n",
        "%%shell\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`\n",
        "wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver\n",
        "mv /tmp/chromedriver /usr/local/bin/chromedriver\n",
        "pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromedriver-autoinstaller"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqvABJmkQf4V",
        "outputId": "b85e2285-722a-44b4-928d-89313076e53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver-autoinstaller) (24.0)\n",
            "Installing collected packages: chromedriver-autoinstaller\n",
            "Successfully installed chromedriver-autoinstaller-0.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEMPLATE"
      ],
      "metadata": {
        "id": "wlNYJ6NwXPdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import chromedriver_autoinstaller\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "# ------------------ Dependencias --------------------------\n",
        "\n",
        "# Configurando o WebDriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # ensure GUI is off\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# definindo o caminho para chromedriver de acordo com sua configuração\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "# configura o webdriver\n",
        "navegador = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "\n",
        "# URL da página alvo\n",
        "url = \"https://www.amazon.com.br/s?k=computa%C3%A7%C3%A3o&i=stripbooks&__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=FIHO8SW3FVIA&sprefix=computa%C3%A7%C3%A3o%2Cstripbooks%2C134&ref=nb_sb_noss_1\"\n",
        "\n",
        "# Lista para armazenar os links dos PDFs e Planilhas\n",
        "pdf_links = []\n",
        "excel_links = []\n",
        "\n",
        "# Obtendo o conteúdo da página\n",
        "navegador.get(url)\n",
        "soup = BeautifulSoup(navegador.page_source, 'html.parser')\n",
        "\n",
        "# Encontrando todos os links na página\n",
        "links = soup.find_all(\"body\")\n",
        "\n",
        "print(links)\n",
        "\n",
        "navegador.quit()\n"
      ],
      "metadata": {
        "id": "dzGuaLBRQsIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# teste"
      ],
      "metadata": {
        "id": "Z8OXOia9gNRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "import chromedriver_autoinstaller\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Configurando o WebDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')  # Execução sem interface gráfica (headless)\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "#  \"disfarçando\" o navegador automatizado\n",
        "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537\")\n",
        "\n",
        "# Instalando o chromedriver\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "# Inicializando o webdriver\n",
        "navegador = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# URL da página alvo\n",
        "url = \"https://www.amazon.com.br/s?rh=n%3A5559875011&fs=true&ref=lp_5559875011_sar\"\n",
        "\n",
        "# Acessando a página\n",
        "navegador.get(url)\n",
        "sleep(5)  # Aguardando o carregamento da página\n",
        "\n",
        "# Esperar até que os resultados sejam carregados\n",
        "try:\n",
        "    element = WebDriverWait(navegador, 10).until(\n",
        "        EC.presence_of_element_located((By.XPATH, \"//div[@data-component-type='s-search-result']\"))\n",
        "    )\n",
        "except:\n",
        "    print(\"Timeout, elementos não carregados\")\n",
        "\n",
        "# Obtendo o HTML da página\n",
        "soup = BeautifulSoup(navegador.page_source, 'html.parser')\n",
        "\n",
        "# Encontrando os containers dos produtos\n",
        "containers = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
        "\n",
        "dados_livros = []\n",
        "\n",
        "for container in containers:\n",
        "    # Buscando o nome do livro\n",
        "    nome_livro = container.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\")\n",
        "    # Buscando o nome do autor\n",
        "    autor = container.find_all(\"span\", class_=\"a-size-base\")[1]\n",
        "    # Buscando o preço do livro\n",
        "    preco = container.find(\"span\", class_=\"a-price\").find(\"span\", class_=\"a-offscreen\")\n",
        "\n",
        "    if nome_livro and autor and preco:\n",
        "        dados_livros.append({\n",
        "            'Nome do Livro': nome_livro.text.strip(),\n",
        "            'Autor': autor.text.strip(),\n",
        "            'Preço': preco.text.strip()\n",
        "        })\n",
        "\n",
        "# Criando e salvando a planilha\n",
        "df = pd.DataFrame(dados_livros)\n",
        "df.to_excel('livros_amazon.xlsx', index = False)\n",
        "\n",
        "# Fechando o navegador\n",
        "navegador.quit()\n"
      ],
      "metadata": {
        "id": "MgFw0GAIkifr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# todas as paginas"
      ],
      "metadata": {
        "id": "-EhIeI9mogIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "import chromedriver_autoinstaller\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Configurando o WebDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')  # Execução sem interface gráfica (headless)\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537\")\n",
        "\n",
        "# Instalando o chromedriver\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "# Inicializando o webdriver\n",
        "navegador = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# URL da página alvo\n",
        "url = \"https://www.amazon.com.br/s?rh=n%3A5559875011&fs=true&ref=lp_5559875011_sar\"\n",
        "\n",
        "# Acessando a página\n",
        "navegador.get(url)\n",
        "sleep(5)  # Aguardando o carregamento da página\n",
        "\n",
        "dados_livros = []\n",
        "\n",
        "while True:\n",
        "    # Esperar até que os resultados sejam carregados\n",
        "    try:\n",
        "        element = WebDriverWait(navegador, 10).until(\n",
        "            EC.presence_of_element_located((By.XPATH, \"//div[@data-component-type='s-search-result']\"))\n",
        "        )\n",
        "    except:\n",
        "        print(\"Timeout, elementos não carregados\")\n",
        "        break\n",
        "\n",
        "    # Obtendo o HTML da página\n",
        "    soup = BeautifulSoup(navegador.page_source, 'html.parser')\n",
        "\n",
        "    # Encontrando os containers dos produtos\n",
        "    containers = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
        "\n",
        "    for container in containers:\n",
        "        # Buscando o nome do livro\n",
        "        nome_livro = container.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\")\n",
        "        # Buscando o nome do autor\n",
        "        autor = container.find(\"span\", class_=\"a-size-base a-color-secondary\")\n",
        "        # Buscando o preço do livro\n",
        "        preco = container.find(\"span\", class_=\"a-price\")\n",
        "\n",
        "        if nome_livro and autor and preco:\n",
        "            dados_livros.append({\n",
        "                'Nome do Livro': nome_livro.text.strip(),\n",
        "                'Autor': autor.text.strip(),\n",
        "                'Preço': preco.text.strip()\n",
        "            })\n",
        "\n",
        "    # Verificar se há um próximo botão de paginação e clicar nele\n",
        "    try:\n",
        "        next_button = navegador.find_element(By.XPATH, \"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-button-accessibility s-pagination-separator']\")\n",
        "        if next_button:\n",
        "            next_button.click()\n",
        "            sleep(5)  # Aguardando o carregamento da próxima página\n",
        "    except:\n",
        "        print(\"Não há mais páginas para carregar.\")\n",
        "        break\n",
        "\n",
        "# Criando e salvando a planilha\n",
        "df = pd.DataFrame(dados_livros)\n",
        "df.to_excel('livros_amazon.xlsx', index=False)\n",
        "\n",
        "print('Dados dos livros capturados e salvos na planilha \"livros_amazon.xlsx\" com sucesso!')\n",
        "\n",
        "# Fechando o navegador\n",
        "navegador.quit()\n"
      ],
      "metadata": {
        "id": "e5RpcsDmoj0T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}